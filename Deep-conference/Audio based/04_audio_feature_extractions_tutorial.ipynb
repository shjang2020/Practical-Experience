{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Audio Feature Extractions\n",
    "=========================\n",
    "\n",
    "``torchaudio`` implements feature extractions commonly used in the audio\n",
    "domain. They are available in ``torchaudio.functional`` and\n",
    "``torchaudio.transforms``.\n",
    "\n",
    "``functional`` implements features as standalone functions.\n",
    "They are stateless.\n",
    "\n",
    "``transforms`` implements features as objects,\n",
    "using implementations from ``functional`` and ``torch.nn.Module``.\n",
    "They can be serialized using TorchScript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (1.9.0)\n",
      "Collecting torch\n",
      "  Downloading torch-1.12.0-cp38-cp38-win_amd64.whl (161.9 MB)\n",
      "     -------------------------------------- 161.9/161.9 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torchvision in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.13.0-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 4.8 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'C:\\\\Users\\\\JangSeongHyun\\\\anaconda3\\\\Lib\\\\site-packages\\\\~~rch\\\\lib\\\\asmjit.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.12.0-cp38-cp38-win_amd64.whl (969 kB)\n",
      "     -------------------------------------- 969.5/969.5 kB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: requests in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (from torchvision) (2.24.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (from torchvision) (1.22.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jangseonghyun\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.0.4)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0\n",
      "    Uninstalling torch-1.9.0:\n",
      "      Successfully uninstalled torch-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.12.0+cpu torchvision==0.8.1+cpu torchaudio===0..0 -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation\n",
    "-----------\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>When running this tutorial in Google Colab, install the required packages\n",
    "\n",
    "   .. code::\n",
    "\n",
    "      !pip install librosa</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'download_asset' from 'torchaudio.utils' (C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\torchaudio\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-aa01962e2a4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload_asset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'download_asset' from 'torchaudio.utils' (C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\torchaudio\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "SAMPLE_SPEECH = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\n",
    "\n",
    "\n",
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "    axes.grid(True)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "def plot_fbank(fbank, title=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Filter bank\")\n",
    "    axs.imshow(fbank, aspect=\"auto\")\n",
    "    axs.set_ylabel(\"frequency bin\")\n",
    "    axs.set_xlabel(\"mel bin\")\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of audio features\n",
    "--------------------------\n",
    "\n",
    "The following diagram shows the relationship between common audio features\n",
    "and torchaudio APIs to generate them.\n",
    "\n",
    "![](https://download.pytorch.org/torchaudio/tutorial-assets/torchaudio_feature_extractions.png)\n",
    "\n",
    "\n",
    "For the complete list of available features, please refer to the\n",
    "documentation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectrogram\n",
    "-----------\n",
    "\n",
    "To get the frequency make-up of an audio signal as it varies with time,\n",
    "you can use :py:func:`torchaudio.transforms.Spectrogram`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAMPLE_SPEECH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0a6ec672466f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSPEECH_WAVEFORM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSAMPLE_SPEECH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplot_waveform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPEECH_WAVEFORM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Original waveform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mAudio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPEECH_WAVEFORM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSAMPLE_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SAMPLE_SPEECH' is not defined"
     ]
    }
   ],
   "source": [
    "SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(SAMPLE_SPEECH)\n",
    "\n",
    "plot_waveform(SPEECH_WAVEFORM, SAMPLE_RATE, title=\"Original waveform\")\n",
    "Audio(SPEECH_WAVEFORM.numpy(), rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "\n",
    "# Define transform\n",
    "spectrogram = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform transform\n",
    "spec = spectrogram(SPEECH_WAVEFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(spec[0], title=\"torchaudio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GriffinLim\n",
    "----------\n",
    "\n",
    "To recover a waveform from a spectrogram, you can use ``GriffinLim``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "\n",
    "spec = T.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    ")(SPEECH_WAVEFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "griffin_lim = T.GriffinLim(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_waveform = griffin_lim(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waveform(reconstructed_waveform, SAMPLE_RATE, title=\"Reconstructed\")\n",
    "Audio(reconstructed_waveform, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mel Filter Bank\n",
    "---------------\n",
    "\n",
    ":py:func:`torchaudio.functional.melscale_fbanks` generates the filter bank\n",
    "for converting frequency bins to mel-scale bins.\n",
    "\n",
    "Since this function does not require input audio/features, there is no\n",
    "equivalent transform in :py:func:`torchaudio.transforms`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 256\n",
    "n_mels = 64\n",
    "sample_rate = 6000\n",
    "\n",
    "mel_filters = F.melscale_fbanks(\n",
    "    int(n_fft // 2 + 1),\n",
    "    n_mels=n_mels,\n",
    "    f_min=0.0,\n",
    "    f_max=sample_rate / 2.0,\n",
    "    sample_rate=sample_rate,\n",
    "    norm=\"slaney\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fbank(mel_filters, \"Mel Filter Bank - torchaudio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison against librosa\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "For reference, here is the equivalent way to get the mel filter bank\n",
    "with ``librosa``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_filters_librosa = librosa.filters.mel(\n",
    "    sr=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    n_mels=n_mels,\n",
    "    fmin=0.0,\n",
    "    fmax=sample_rate / 2.0,\n",
    "    norm=\"slaney\",\n",
    "    htk=True,\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fbank(mel_filters_librosa, \"Mel Filter Bank - librosa\")\n",
    "\n",
    "mse = torch.square(mel_filters - mel_filters_librosa).mean().item()\n",
    "print(\"Mean Square Difference: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MelSpectrogram\n",
    "--------------\n",
    "\n",
    "Generating a mel-scale spectrogram involves generating a spectrogram\n",
    "and performing mel-scale conversion. In ``torchaudio``,\n",
    ":py:func:`torchaudio.transforms.MelSpectrogram` provides\n",
    "this functionality.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_mels = 128\n",
    "\n",
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm=\"slaney\",\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    ")\n",
    "\n",
    "melspec = mel_spectrogram(SPEECH_WAVEFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel=\"mel freq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison against librosa\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "For reference, here is the equivalent means of generating mel-scale\n",
    "spectrograms with ``librosa``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_librosa = librosa.feature.melspectrogram(\n",
    "    y=SPEECH_WAVEFORM.numpy()[0],\n",
    "    sr=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    n_mels=n_mels,\n",
    "    norm=\"slaney\",\n",
    "    htk=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(melspec_librosa, title=\"MelSpectrogram - librosa\", ylabel=\"mel freq\")\n",
    "\n",
    "mse = torch.square(melspec - melspec_librosa).mean().item()\n",
    "print(\"Mean Square Difference: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2048\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_mels = 256\n",
    "n_mfcc = 256\n",
    "\n",
    "mfcc_transform = T.MFCC(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mfcc=n_mfcc,\n",
    "    melkwargs={\n",
    "        \"n_fft\": n_fft,\n",
    "        \"n_mels\": n_mels,\n",
    "        \"hop_length\": hop_length,\n",
    "        \"mel_scale\": \"htk\",\n",
    "    },\n",
    ")\n",
    "\n",
    "mfcc = mfcc_transform(SPEECH_WAVEFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(mfcc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison against librosa\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec = librosa.feature.melspectrogram(\n",
    "    y=SPEECH_WAVEFORM.numpy()[0],\n",
    "    sr=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    n_mels=n_mels,\n",
    "    htk=True,\n",
    "    norm=None,\n",
    ")\n",
    "\n",
    "mfcc_librosa = librosa.feature.mfcc(\n",
    "    S=librosa.core.spectrum.power_to_db(melspec),\n",
    "    n_mfcc=n_mfcc,\n",
    "    dct_type=2,\n",
    "    norm=\"ortho\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(mfcc_librosa)\n",
    "\n",
    "mse = torch.square(mfcc - mfcc_librosa).mean().item()\n",
    "print(\"Mean Square Difference: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LFCC\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2048\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_lfcc = 256\n",
    "\n",
    "lfcc_transform = T.LFCC(\n",
    "    sample_rate=sample_rate,\n",
    "    n_lfcc=n_lfcc,\n",
    "    speckwargs={\n",
    "        \"n_fft\": n_fft,\n",
    "        \"win_length\": win_length,\n",
    "        \"hop_length\": hop_length,\n",
    "    },\n",
    ")\n",
    "\n",
    "lfcc = lfcc_transform(SPEECH_WAVEFORM)\n",
    "plot_spectrogram(lfcc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pitch\n",
    "-----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch = F.detect_pitch_frequency(SPEECH_WAVEFORM, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pitch(waveform, sr, pitch):\n",
    "    figure, axis = plt.subplots(1, 1)\n",
    "    axis.set_title(\"Pitch Feature\")\n",
    "    axis.grid(True)\n",
    "\n",
    "    end_time = waveform.shape[1] / sr\n",
    "    time_axis = torch.linspace(0, end_time, waveform.shape[1])\n",
    "    axis.plot(time_axis, waveform[0], linewidth=1, color=\"gray\", alpha=0.3)\n",
    "\n",
    "    axis2 = axis.twinx()\n",
    "    time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "    axis2.plot(time_axis, pitch[0], linewidth=2, label=\"Pitch\", color=\"green\")\n",
    "\n",
    "    axis2.legend(loc=0)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "plot_pitch(SPEECH_WAVEFORM, SAMPLE_RATE, pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaldi Pitch (beta)\n",
    "------------------\n",
    "\n",
    "Kaldi Pitch feature [1] is a pitch detection mechanism tuned for automatic\n",
    "speech recognition (ASR) applications. This is a beta feature in ``torchaudio``,\n",
    "and it is available as :py:func:`torchaudio.functional.compute_kaldi_pitch`.\n",
    "\n",
    "1. A pitch extraction algorithm tuned for automatic speech recognition\n",
    "\n",
    "   Ghahremani, B. BabaAli, D. Povey, K. Riedhammer, J. Trmal and S.\n",
    "   Khudanpur\n",
    "\n",
    "   2014 IEEE International Conference on Acoustics, Speech and Signal\n",
    "   Processing (ICASSP), Florence, 2014, pp. 2494-2498, doi:\n",
    "   10.1109/ICASSP.2014.6854049.\n",
    "   [`abstract <https://ieeexplore.ieee.org/document/6854049>`__],\n",
    "   [`paper <https://danielpovey.com/files/2014_icassp_pitch.pdf>`__]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_feature = F.compute_kaldi_pitch(SPEECH_WAVEFORM, SAMPLE_RATE)\n",
    "pitch, nfcc = pitch_feature[..., 0], pitch_feature[..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kaldi_pitch(waveform, sr, pitch, nfcc):\n",
    "    _, axis = plt.subplots(1, 1)\n",
    "    axis.set_title(\"Kaldi Pitch Feature\")\n",
    "    axis.grid(True)\n",
    "\n",
    "    end_time = waveform.shape[1] / sr\n",
    "    time_axis = torch.linspace(0, end_time, waveform.shape[1])\n",
    "    axis.plot(time_axis, waveform[0], linewidth=1, color=\"gray\", alpha=0.3)\n",
    "\n",
    "    time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "    ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label=\"Pitch\", color=\"green\")\n",
    "    axis.set_ylim((-1.3, 1.3))\n",
    "\n",
    "    axis2 = axis.twinx()\n",
    "    time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n",
    "    ln2 = axis2.plot(time_axis, nfcc[0], linewidth=2, label=\"NFCC\", color=\"blue\", linestyle=\"--\")\n",
    "\n",
    "    lns = ln1 + ln2\n",
    "    labels = [l.get_label() for l in lns]\n",
    "    axis.legend(lns, labels, loc=0)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "plot_kaldi_pitch(SPEECH_WAVEFORM, SAMPLE_RATE, pitch, nfcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
